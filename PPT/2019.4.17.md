# Today's goal write the resnet!

Pre-trained models present in Keras

- 保存模型：保存是为了可以方便的迁移学习，把网络结构和权重分开保存，当然也可以直接一起保存，需要的导入： from keras.models import model_from_yaml, load_model 

深度学习工程师50%的时间在调参数，49%的时间在对抗过/欠拟合，剩下1%时间在修改网上down下来的程序





train_generator = train_datagen.flow_from_directory(
        '/.../train', 
        target_size=(150, 150),  # all images will be resized to 150x150
        batch_size=32,
        class_mode='categorical')                       # matt，多分类

validation_generator = test_datagen.flow_from_directory(
        '/.../validation',
        target_size=(150, 150),
        batch_size=32,
        class_mode='categorical')                      # matt，多分类 
        class_mode='binary



DM train : val : test= 33983  5664 11328

DR train : val : test=





要回答这个问题，首先需要简单谈一下keras的历史。keras是François Chollet于2014-2015年开始编写的开源高层深度学习API。所谓“高层”，是相对于“底层”运算而言（例如add，matmul，transpose，conv2d）。keras把这些底层运算封装成一些常用的神经网络模块类型（例如Dense, Conv2D, LSTM等），再加上像Model、Optimizer等的一些的抽象，来增强API的易用性和代码的可读性，提高深度学习开发者编写模型的效率。keras本身并不具备底层运算的能力，所以它需要和一个具备这种底层运算能力的backend（后端）协同工作。keras的特性之一就是**可以互换的后端**，你在所有后端上写的keras代码都是一样的。从一个后端训练并存储的模型，可以用别的后端加载并运行。

keras最初发行的时候，tensorflow还没有开源（fchollet开始写keras的时候还未加入Google）。那时的keras主要使用的是Theano后端。2015年底TensorFlow开源后，keras才开始搭建TensorFlow后端。今天TensorFlow是keras最常用的后端。

2016－2017年间，Google Brain组根据开源用户对TensorFlow易用性的反馈，决定采纳keras为首推、并内置支持的高层API。当时TensorFlow已经有tf.estimator、slim、sonnet、TensorLayers等诸多高层次API，选择keras主要是考虑它的优秀性以及在用户群中的受欢迎程度，不过那个是另一个故事线就不展开说了。

所以，keras的代码被逐渐吸收进入tensorflow的代码库，那时fchollet也加入了Google Brain组。所以就产生了**tf.keras：一个不强调后端可互换性、和tensorflow更紧密整合、得到tensorflow其他组建更好支持、且符合keras标准的高层次API**。



## **Epoch、Batch Size、和迭代**

2274/2274 [==============================] - 857s 377ms/step - loss: 0.3735 - acc: 0.8239 - val_loss: 0.3766 - val_acc: 0.8063
Epoch 38/500
2274/2274 [==============================] - 862s 379ms/step - loss: 0.3712 - acc: 0.8244 - val_loss: 0.3770 - val_acc: 0.8000
Epoch 39/500
2274/2274 [==============================] - 863s 380ms/step - loss: 0.3712 - acc: 0.8252 - val_loss: 0.3760 - val_acc: 0.8094
Epoch 40/500
2274/2274 [==============================] - 864s 380ms/step - loss: 0.3682 - acc: 0.8265 - val_loss: 0.3758 - val_acc: 0.8031
Epoch 41/500
2274/2274 [==============================] - 868s 382ms/step - loss: 0.3657 - acc: 0.8284 - val_loss: 0.3741 - val_acc: 0.8000
Epoch 42/500
2274/2274 [==============================] - 876s 385ms/step - loss: 0.3665 - acc: 0.8270 - val_loss: 0.3789 - val_acc: 0.7875
Epoch 43/500
2274/2274 [==============================] - 885s 389ms/step - loss: 0.3670 - acc: 0.8294 - val_loss: 0.3729 - val_acc: 0.8031
Epoch 44/500
2274/2274 [==============================] - 869s 382ms/step - loss: 0.3665 - acc: 0.8286 - val_loss: 0.3800 - val_acc: 0.7969
Epoch 45/500
2274/2274 [==============================] - 882s 388ms/step - loss: 0.3657 - acc: 0.8298 - val_loss: 0.3782 - val_acc: 0.7906
Epoch 46/500
2274/2274 [==============================] - 867s 381ms/step - loss: 0.3646 - acc: 0.8288 - val_loss: 0.3756 - val_acc: 0.7969
('loss', 0.37698638439178467)
('acc', 0.84375)









Epoch 1/100
1137/1137 [==============================] - 26139s 23s/step - loss: 0.6811 - acc: 0.5916 - val_loss: 0.5974 - val_acc: 0.6594
Epoch 2/100
1137/1137 [==============================] - 26013s 23s/step - loss: 0.5763 - acc: 0.6860 - val_loss: 0.5436 - val_acc: 0.7000
Epoch 3/100
1137/1137 [==============================] - 26035s 23s/step - loss: 0.5367 - acc: 0.7146 - val_loss: 0.5173 - val_acc: 0.7063
Epoch 4/100
1137/1137 [==============================] - 25990s 23s/step - loss: 0.5175 - acc: 0.7275 - val_loss: 0.4960 - val_acc: 0.7156
Epoch 5/100
1137/1137 [==============================] - 25977s 23s/step - loss: 0.5019 - acc: 0.7370 - val_loss: 0.4889 - val_acc: 0.7406
Epoch 6/100

