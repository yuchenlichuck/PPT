---
typora-copy-images-to: image
typora-root-url: image
---

<center>神马汉斯</center>

![1552908417757](1552908417757.png)

终身学习

catastrophic learning

![1552908734107](/1552908734107.png)

![1552911109776](/1552911109776.png)

automated identification of diabetic retinopathy using deep learning 

**resnet**



![1552924948214](/1552924948214.png)

![1553092488793](/1553092488793.png)



<center>
    Residual network
</center>

**1. 卷积核（convolutional kernel）**：可以看作对某个局部的加权求和；它是对应局部感知，它的原理是在观察某个物体时我们既不能观察每个像素也不能一次观察整体，而是先从局部开始认识，这就对应了卷积。卷积核的大小一般有1x1,3x3和5x5的尺寸（一般是奇数x奇数）。



卷积核的个数就对应输出的通道数（channels），这里需要说明的是对于输入的每个通道，输出每个通道上的卷积核是不一样的。比如输入是28x28x192(WxDxK,K代表通道数)，然后在3x3的卷积核，卷积通道数为128，那么卷积的参数有3x3x192x128，其中前两个对应的每个卷积里面的参数，后两个对应的卷积总的个数（一般理解为，卷积核的权值共享只在每个单独通道上有效，至于通道与通道间的对应的卷积核是独立不共享的，所以这里是192x128）。



池化（pooling）：卷积特征往往对应某个局部的特征。要得到global的特征需要将全局的特征执行一个aggregation（聚合）。池化就是这样一个操作，对于每个卷积通道，将更大尺寸（甚至是global）上的卷积特征进行pooling就可以得到更有全局性的特征。这里的pooling当然就对应了cross region。



1 x 1卷积核，网中网

让我们看一下真正work的示例。当输入为6x6x32时，1x1卷积的形式是1x1x32，当只有一个1x1卷积核的时候，此时输出为6x6x1。此时便可以体会到1x1卷积的实质作用：降维。当1x1卷积核的个数小于输入channels数量时，即降维[3]。

![img](https://pic4.zhimg.com/80/v2-9fa17784edcb8483099e95920799c357_hd.jpg)



full connection

![img](https://pic3.zhimg.com/80/v2-495ba8088403c1d4374cdf77dbb104a6_hd.jpg)



**跨通道信息交互（channal 的变换）**

例子：使用1x1卷积核，实现降维和升维的操作其实就是channel间信息的线性组合变化，3x3，64channels的卷积核后面添加一个1x1，28channels的卷积核，就变成了3x3，28channels的卷积核，原来的64个channels就可以理解为跨通道线性组合变成了28channels，这就是通道间的信息交互[7]。



注意：只是在channel维度上做线性组合，W和H上是共享权值的sliding window

预处理方式





DR:DM

3499:11210





#### 2) EasyEnsemble 和 BalanceCascade

EasyEnsemble和BalanceCascade采用集成学习机制来处理传统随机欠采样中的信息丢失问题。

- **EasyEnsemble**将多数类样本随机划分成n个子集，每个子集的数量等于少数类样本的数量，这相当于欠采样。接着将每个子集与少数类样本结合起来分别训练一个模型，最后将n个模型集成，这样虽然每个子集的样本少于总体样本，但集成后总信息量并不减少。

- 如果说EasyEnsemble是基于无监督的方式从多数类样本中生成子集进行欠采样，那么**BalanceCascade**则是采用了有监督结合Boosting的方式。在第n轮训练中，将从多数类样本中抽样得来的子集与少数类样本结合起来训练一个基学习器H，训练完后多数类中能被H正确分类的样本会被剔除。在接下来的第n+1轮中，从被剔除后的多数类样本中产生子集用于与少数类样本结合起来训练，最后将不同的基学习器集成起来。BalanceCascade的有监督表现在每一轮的基学习器起到了在多数类中选择样本的作用，而其Boosting特点则体现在每一轮丢弃被正确分类的样本，进而后续基学习器会更注重那些之前分类错误的样本。



![1553151741282](/1553151741282.png)

![1553151842553](/1553151842553.png)

![1553152271353](/1553152271353.png)

![1553152844753](/1553152844753.png)

![1553153049231](/1553153049231.png)

![1553153167441](/1553153167441.png)

![1553153209711](/1553153209711.png)

```python
def flip(img,axes):
    if (axes == 0) :
        #horizental flip
        return cv2.flip( img, 0 )
    elif(axes == 1):
        #vertical flip
        return cv2.flip( img, 1 )
    elif(axes == -1):
        #both direction
        return cv2.flip( img, -1 ) 
bflp = flip(img,-1)
plt.imshow(bflp)
```

11200/3499=3.2009



- 局部裁剪，翻转、旋转
- 亮度，对比度，Gamma
- 色相，饱和度

同一幅图像经过一种变换、或者多种变换的组合会生成一幅新的图像，更进一步，变换的位置、方向、变化量级是随机的，进而增加了训练样本的数量。

https://yinguobing.com/short-of-images-for-deep-learning/

https://blog.csdn.net/weixin_41803874/article/details/81201699

卷积操作本身只具有平移不变性，不具有旋转不变性。

灵敏性与假阴性率（漏诊率），特异性与[假阳性](https://www.baidu.com/s?wd=%E5%81%87%E9%98%B3%E6%80%A7&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)率（误诊率）

```
灵敏度（也称真阳性率，sensitivity）=真阳性人数/（真阳性人数+假阴性人数）*100%。
```

指正确判断病人的程度，也即实际有病而被正确诊断的百分比。

![这里写图片描述](http://img.voidcn.com/vcimg/000/005/145/387_3be_362.jpg)

```
特异度（也称真阴性率，specificity）=真阴性人数/（真阴性人数+假阳性人数））*100%。
```