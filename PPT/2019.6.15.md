![1560578924423](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\1560578924423.png)

Recurrent Neural Network



slot filling<destination,time of  arrival



1-of-n encoding



dimension for other, word hashing

![1560580770532](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\1560580770532.png)

The output of hidden layer are stored in the memory.

Memory can be considered as another input

![1560581384531](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\1560581384531.png)

![1560583263902](E:\github\PPT\image\1560583263902.png)

![1560583433989](E:\github\PPT\image\1560583433989.png)

![1560583904766](E:\github\PPT\image\1560583904766.png)

![1560584500941](E:\github\PPT\image\1560584500941.png)

![1560584888286](E:\github\PPT\image\1560584888286.png)

![1560585100502](E:\github\PPT\image\1560585100502.png)

![1560585172920](E:\github\PPT\image\1560585172920.png)

Long short-term memory (LSTM)



can deal with gradient vanishing (not gradient explode)

add，forget gate

no gradient vanishing

Gated recurrent unit (GRU)

​	simpler  thant LSTM

![1560585811264](E:\github\PPT\image\1560585811264.png)

speech recognition

Syntactic parsing.



![1560587239441](E:\github\PPT\image\1560587239441.png)

deep and structures.

