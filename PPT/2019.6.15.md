![1560578924423](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\1560578924423.png)

Recurrent Neural Network



slot filling<destination,time of  arrival



1-of-n encoding



dimension for other, word hashing

![1560580770532](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\1560580770532.png)

The output of hidden layer are stored in the memory.

Memory can be considered as another input

![1560581384531](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\1560581384531.png)

![1560583263902](E:\github\PPT\image\1560583263902.png)

![1560583433989](E:\github\PPT\image\1560583433989.png)

![1560583904766](E:\github\PPT\image\1560583904766.png)

![1560584500941](E:\github\PPT\image\1560584500941.png)

![1560584888286](E:\github\PPT\image\1560584888286.png)

![1560585100502](E:\github\PPT\image\1560585100502.png)

![1560585172920](E:\github\PPT\image\1560585172920.png)

Long short-term memory (LSTM)



can deal with gradient vanishing (not gradient explode)

add，forget gate

no gradient vanishing

Gated recurrent unit (GRU)

​	simpler  thant LSTM

![1560585811264](E:\github\PPT\image\1560585811264.png)

speech recognition

Syntactic parsing.



![1560587239441](E:\github\PPT\image\1560587239441.png)

deep and structures.

```
Epoch 63/100
351/351 [==============================] - 36s 102ms/step - loss: 0.3495 - acc: 0.8412
2275/2275 [==============================] - 846s 372ms/step - loss: 0.3403 - acc: 0.8445 - val_loss: 0.3495 - val_acc: 0.8412
Epoch 64/100
351/351 [==============================] - 36s 102ms/step - loss: 0.3488 - acc: 0.8411
2275/2275 [==============================] - 845s 371ms/step - loss: 0.3406 - acc: 0.8437 - val_loss: 0.3488 - val_acc: 0.8411
Epoch 65/100
351/351 [==============================] - 36s 102ms/step - loss: 0.3520 - acc: 0.8382
2275/2275 [==============================] - 845s 371ms/step - loss: 0.3379 - acc: 0.8452 - val_loss: 0.3520 - val_acc: 0.8382
Epoch 66/100
351/351 [==============================] - 36s 102ms/step - loss: 0.3522 - acc: 0.8400
2275/2275 [==============================] - 845s 371ms/step - loss: 0.3411 - acc: 0.8439 - val_loss: 0.3522 - val_acc: 0.8400
Epoch 67/100
351/351 [==============================] - 36s 102ms/step - loss: 0.3490 - acc: 0.8415
2275/2275 [==============================] - 845s 371ms/step - loss: 0.3397 - acc: 0.8437 - val_loss: 0.3490 - val_acc: 0.8415
('loss', 0.3413906427214557)
('acc', 0.8464971)
```

Epoch 38/100
2274/2274 [==============================] - 694s 305ms/step - loss: 0.3717 - acc: 0.8279 - val_loss: 0.3654 - val_acc: 0.8219
Epoch 39/100
2274/2274 [==============================] - 691s 304ms/step - loss: 0.3712 - acc: 0.8258 - val_loss: 0.3667 - val_acc: 0.8156
Epoch 40/100
2274/2274 [==============================] - 702s 309ms/step - loss: 0.3713 - acc: 0.8246 - val_loss: 0.3674 - val_acc: 0.8187
Epoch 41/100
2274/2274 [==============================] - 687s 302ms/step - loss: 0.3717 - acc: 0.8245 - val_loss: 0.3679 - val_acc: 0.8187
('loss', 0.2663668096065521)
('acc', 0.84375)







Kaggle images



53576

35126

《竞赛往事——我该如何很好地告别竞赛》

今天是2019年的6月22日，星期六。两年前的这一天，对我来说是一个转折点，这一天， 中国科学技术大学自主招生名单公布。我，落榜了。知道成绩的一刻，没有太难过只是，叫了一声，次奥，数学一道大题都没对的吗。我也许那一刻心理想的是，当年杨甬的数竞课，为啥不好好听呢。一整个下午都是一种萎靡的状态，姐姐把我叫了出来，跟我提醒到：“虽然你自招没有考好，但是你必须做好你高考也考的不好的准备，开始填志愿吧。”姐姐带着高考志愿填报指南和我一起去公园散步。看着志愿书，学校从高到低看了一遍，凭着不确定的估分，随手选了几个学校。顿时很惭愧，这些都是我自招时，看都没看的学校。

晚上和家里人吃完了饭，心理还是很难过。想了想，离开了家门，“去科大看看吧，从南到北”，父亲不放心，也跟着我一起去。走进了东校区，就像我第一次走进科大一样，寻找第一次来科大时的路线看着大学生生活是向往。十八层的理化大楼，依然能把月亮遮住，，亮着灯的网球场，是否有我约好的朋友在等着我打球呢，第三教学楼的，依然灯火通明，在那里是否还有一位少年在那里刷着金考卷呢，从东区，中区到西区，不免满怀感慨，一切都这么熟悉，但这里不属于我。我想着我再也不会来科大，但是实际上，我在第二年的寒假，受小河的邀请，又来到了科大，听着给我介绍我熟悉的科大。是的，你是在合肥长大的，可是你并不一定能上合肥的大学。

​	



我承认我曾经精神科大人了很长一段时间，我也一直走不出这个结，无法面对这次失利。

我对科大是有感情的，我至今仍然认为，科大是中国唯一一个能容得下安静的书桌的地方。

但关于这段经历究竟能给我带来什么呢

隔壁老王说过一段话：很多事一开始看起来非常痛苦，但是其实以后一看，都是一个不错的人生经历。。就是这样。一时成败得失又能怎样呢，过眼云烟罢了，重要的是奋斗的过程，只要不懈努力找到正确方法，相信前途是光明的。