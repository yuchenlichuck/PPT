DM:DR=22420:20994

G:\data\DM

G:\data\DR

*æœªå¤„ç†é”™åˆ†ç±»*

è¿›è¡Œäº†å¤åˆ¶

å‘ç°æœ‰ä¸å°‘äº®åº¦å¯ä»¥çš„ç…§ç‰‡

--images=

3.24ã€

planï¼šè·‘kaggleæ•°æ®é›†



![TensorBoard CNN](https://github.com/yuchenlichuck/dsi-capstone/raw/master/images/readme/cnn_two_classes_tensorboard.png)





3.24

net design 



slim: netåº“

def brightness(img):
    value = random.randint(-20, 20)

    if not value: value = random.randint(-30, 20)
    
    if value >= 0:
        return np.where((255 - img) < value,255,img+value)
    else:
        return np.where(img < value ,0,img-value)





Above we made the mistake of testing our data on the same set of data that was used for training. **This is not generally a good idea**. If we optimize our estimator this way, we will tend to **over-fit** the data: that is, we learn the noise.

A better way to test a model is to use a hold-out set which doesn't enter the training. We've seen this before using scikit-learn's train/test split utility:

[@Ranjan-mn](https://github.com/Ranjan-mn) As I was trying to load the 20Gb's of `.npy` file into RAM but when `cnn.py` converts the array into `float32` I ran out of memory as it requires more than 61Gb's of RAM to hold the 20Gb's of float32 array. So, now I have to either opt for AWS or GCP with higher RAM configurations to train the whole network at once. I suggest you, use **Transfer learning** on either *VGG16* or *Inception-v3* as it will help to improve accuracy. [Link For Transfer Learning Example](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)

ğŸ‘ 1

<details class="details-overlay details-reset position-relative float-left reaction-popover-container js-reaction-popover-container" style="box-sizing: border-box; display: inline-block; position: relative !important; float: left !important; z-index: 100;"><summary class="btn-link reaction-summary-item add-reaction-btn" aria-label="Add your reaction" aria-haspopup="menu" style="box-sizing: border-box; display: inline-block; cursor: pointer; -webkit-appearance: none; background-color: transparent; border: 0px; color: rgb(3, 102, 214); font-size: inherit; padding: 9px 15px 7px; text-decoration: none; user-select: none; white-space: nowrap; opacity: 0; transition: opacity 0.1s ease-in-out 0s; float: left; line-height: 18px; list-style: none;"><svg class="octicon octicon-plus-small add-reaction-plus-icon" viewBox="0 0 7 16" version="1.1" width="7" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 4H3v3H0v1h3v3h1V8h3V7H4V4z"></path></svg><span>&nbsp;</span><svg class="octicon octicon-smiley" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8s3.58 8 8 8 8-3.58 8-8-3.58-8-8-8zm4.81 12.81a6.72 6.72 0 0 1-2.17 1.45c-.83.36-1.72.53-2.64.53-.92 0-1.81-.17-2.64-.53-.81-.34-1.55-.83-2.17-1.45a6.773 6.773 0 0 1-1.45-2.17A6.59 6.59 0 0 1 1.21 8c0-.92.17-1.81.53-2.64.34-.81.83-1.55 1.45-2.17.62-.62 1.36-1.11 2.17-1.45A6.59 6.59 0 0 1 8 1.21c.92 0 1.81.17 2.64.53.81.34 1.55.83 2.17 1.45.62.62 1.11 1.36 1.45 2.17.36.83.53 1.72.53 2.64 0 .92-.17 1.81-.53 2.64-.34.81-.83 1.55-1.45 2.17zM4 6.8v-.59c0-.66.53-1.19 1.2-1.19h.59c.66 0 1.19.53 1.19 1.19v.59c0 .67-.53 1.2-1.19 1.2H5.2C4.53 8 4 7.47 4 6.8zm5 0v-.59c0-.66.53-1.19 1.2-1.19h.59c.66 0 1.19.53 1.19 1.19v.59c0 .67-.53 1.2-1.19 1.2h-.59C9.53 8 9 7.47 9 6.8zm4 3.2c-.72 1.88-2.91 3-5 3s-4.28-1.13-5-3c-.14-.39.23-1 .66-1h8.59c.41 0 .89.61.75 1z"></path></svg></summary></details>


  